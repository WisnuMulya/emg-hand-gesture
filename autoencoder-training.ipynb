{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b9ca6116-d001-49c7-a17d-91b0465e30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f4c56-1e73-47e0-b02e-3a786011b05d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05f182-0b28-49f0-90b2-3eaccb2897eb",
   "metadata": {},
   "source": [
    "## Create EMG Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f3712-c6fa-4b86-bc86-5c396dbcad05",
   "metadata": {},
   "source": [
    "### Load Dataset Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab07c9f9-2426-4c18-a30a-96e1f0732302",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('df_hudgin.pkl', 'rb')\n",
    "df = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef6a314-18ea-4ac6-a7de-576eb4cb1807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>day</th>\n",
       "      <th>session</th>\n",
       "      <th>motion</th>\n",
       "      <th>repetition</th>\n",
       "      <th>window_emg</th>\n",
       "      <th>mav</th>\n",
       "      <th>wl</th>\n",
       "      <th>ssc</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub03</td>\n",
       "      <td>D3</td>\n",
       "      <td>S1</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.4140625, 0.6015625, -0.203125, -0.2109375,...</td>\n",
       "      <td>[0.26875, 0.1434375, 0.17390625, 0.03375, 0.03...</td>\n",
       "      <td>[18.875, 10.3515625, 14.3125, 2.296875, 2.1562...</td>\n",
       "      <td>[29, 27, 40, 29, 26, 29, 34, 42]</td>\n",
       "      <td>[24, 20, 30, 27, 18, 25, 35, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub03</td>\n",
       "      <td>D3</td>\n",
       "      <td>S1</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.2421875, -0.0546875, -0.0078125, 0.015625,...</td>\n",
       "      <td>[0.2403125, 0.09796875, 0.135, 0.0196875, 0.02...</td>\n",
       "      <td>[18.3125, 6.9453125, 12.1015625, 1.4375, 2.218...</td>\n",
       "      <td>[33, 31, 41, 29, 33, 28, 37, 34]</td>\n",
       "      <td>[28, 19, 35, 21, 26, 25, 36, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub03</td>\n",
       "      <td>D3</td>\n",
       "      <td>S1</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.5546875, -0.125, -0.0625, -0.0234375, -0....</td>\n",
       "      <td>[0.153125, 0.1021875, 0.130625, 0.01890625, 0....</td>\n",
       "      <td>[11.6640625, 7.6328125, 12.078125, 1.3125, 1.9...</td>\n",
       "      <td>[31, 31, 40, 29, 29, 25, 37, 25]</td>\n",
       "      <td>[29, 26, 37, 18, 25, 22, 29, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub03</td>\n",
       "      <td>D3</td>\n",
       "      <td>S1</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.1953125, -0.0625, -0.265625, -0.0234375, ...</td>\n",
       "      <td>[0.12859375, 0.10546875, 0.10609375, 0.0170312...</td>\n",
       "      <td>[9.609375, 8.34375, 8.875, 1.140625, 1.375, 5....</td>\n",
       "      <td>[30, 32, 37, 34, 22, 28, 34, 30]</td>\n",
       "      <td>[30, 27, 32, 17, 16, 24, 26, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub03</td>\n",
       "      <td>D3</td>\n",
       "      <td>S1</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0703125, -0.1484375, 0.171875, -0.015625, ...</td>\n",
       "      <td>[0.13140625, 0.1171875, 0.08609375, 0.01421875...</td>\n",
       "      <td>[9.59375, 9.6875, 6.8046875, 0.9296875, 1.1796...</td>\n",
       "      <td>[33, 32, 35, 33, 25, 32, 33, 35]</td>\n",
       "      <td>[26, 32, 28, 15, 16, 26, 30, 29]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject day session motion  repetition  \\\n",
       "0   sub03  D3      S1     OH           1   \n",
       "1   sub03  D3      S1     OH           1   \n",
       "2   sub03  D3      S1     OH           1   \n",
       "3   sub03  D3      S1     OH           1   \n",
       "4   sub03  D3      S1     OH           1   \n",
       "\n",
       "                                          window_emg  \\\n",
       "0  [[0.4140625, 0.6015625, -0.203125, -0.2109375,...   \n",
       "1  [[0.2421875, -0.0546875, -0.0078125, 0.015625,...   \n",
       "2  [[-0.5546875, -0.125, -0.0625, -0.0234375, -0....   \n",
       "3  [[-0.1953125, -0.0625, -0.265625, -0.0234375, ...   \n",
       "4  [[0.0703125, -0.1484375, 0.171875, -0.015625, ...   \n",
       "\n",
       "                                                 mav  \\\n",
       "0  [0.26875, 0.1434375, 0.17390625, 0.03375, 0.03...   \n",
       "1  [0.2403125, 0.09796875, 0.135, 0.0196875, 0.02...   \n",
       "2  [0.153125, 0.1021875, 0.130625, 0.01890625, 0....   \n",
       "3  [0.12859375, 0.10546875, 0.10609375, 0.0170312...   \n",
       "4  [0.13140625, 0.1171875, 0.08609375, 0.01421875...   \n",
       "\n",
       "                                                  wl  \\\n",
       "0  [18.875, 10.3515625, 14.3125, 2.296875, 2.1562...   \n",
       "1  [18.3125, 6.9453125, 12.1015625, 1.4375, 2.218...   \n",
       "2  [11.6640625, 7.6328125, 12.078125, 1.3125, 1.9...   \n",
       "3  [9.609375, 8.34375, 8.875, 1.140625, 1.375, 5....   \n",
       "4  [9.59375, 9.6875, 6.8046875, 0.9296875, 1.1796...   \n",
       "\n",
       "                                ssc                                zc  \n",
       "0  [29, 27, 40, 29, 26, 29, 34, 42]  [24, 20, 30, 27, 18, 25, 35, 30]  \n",
       "1  [33, 31, 41, 29, 33, 28, 37, 34]  [28, 19, 35, 21, 26, 25, 36, 28]  \n",
       "2  [31, 31, 40, 29, 29, 25, 37, 25]  [29, 26, 37, 18, 25, 22, 29, 24]  \n",
       "3  [30, 32, 37, 34, 22, 28, 34, 30]  [30, 27, 32, 17, 16, 24, 26, 28]  \n",
       "4  [33, 32, 35, 33, 25, 32, 33, 35]  [26, 32, 28, 15, 16, 26, 30, 29]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d152dc0d-59fb-4ad1-bf0c-ba800c3598c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c782ee46-4016-4b26-9355-37c0a81f7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['window_emg_list'] = df.window_emg.apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c7957fa6-7f2d-4feb-8271-3be817f3bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['window_emg_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3cb4ecbe-3f37-4e72-b797-670724ad2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('df_hudgin_min.pkl', 'wb')\n",
    "pickle.dump(df[['subject', 'day', 'session', 'motion', 'repetition', 'window_emg_list']], file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f6681-35d5-49da-aac7-65209a4b0f54",
   "metadata": {},
   "source": [
    "### Preprocess Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d690cfb-8a29-4515-b92f-61fd204f6f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/wwsz02rn0qn31sz3qslnnjm80000gn/T/ipykernel_66414/4281264563.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.motion.replace(\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df.motion.replace(\n",
    "    df.motion.drop_duplicates().tolist(),\n",
    "    [i for i in range(9)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc86007a-0b16-4585-8f16-50b2d5cb9c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    65100\n",
       "1    65100\n",
       "2    65100\n",
       "3    65100\n",
       "4    65100\n",
       "5    65100\n",
       "6    65100\n",
       "7    65100\n",
       "8    65100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6d3c7-98c5-4dfe-8b78-2d4e2ffc3e57",
   "metadata": {},
   "source": [
    "## Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "648495dc-76d9-4ab9-a498-5bdb853ec7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfa38583-2e46-4515-925a-74a8a70a0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(SSAE, self).__init__()\n",
    "        self.ae1 = Autoencoder(input_dim, hidden_dim1)\n",
    "        self.ae2 = Autoencoder(hidden_dim1, hidden_dim2)\n",
    "        self.classifier = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded1, _ = self.ae1(x)\n",
    "        encoded2, _ = self.ae2(encoded1)\n",
    "        logits = self.classifier(encoded2)\n",
    "        output = self.softmax(logits)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "063be292-b473-4b73-9c6e-b39aae063c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x177e58d60>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssae.ae1.encoder[1].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4cb24bc4-f495-4434-81d3-7a140e5d091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f70902d-20d2-47ec-991f-edaae018f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e, d = ssae.ae1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "57eb53ca-c001-4b93-8eb2-e6300db424da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SSAELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b86b20bd-f92a-4da4-8a5f-ecd1f7fed88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5382, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(a, d, e, ssae.ae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "754926ed-9ec2-488a-8541-ef5539045111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSAELoss(nn.Module):\n",
    "    def __init__(self, l2_weight=0.0001, sparsity_weight=0.01, sparsity_target=0.05):\n",
    "        super(SSAELoss, self).__init__()\n",
    "        self.l2_weight = l2_weight\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, x, decoded, encoded, model):\n",
    "        # Mean Square Error loss\n",
    "        mse_loss = self.mse_loss(decoded, x)\n",
    "\n",
    "        # L2 regularization loss\n",
    "        l2_loss = 0\n",
    "        for param in model.parameters():\n",
    "            l2_loss += torch.sum(param ** 2)\n",
    "        l2_loss *= self.l2_weight * .5\n",
    "\n",
    "        # KL divergence sparsity loss\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        rho = torch.full_like(rho_hat, self.sparsity_target)\n",
    "        \n",
    "        # Prepare inputs for KLDivLoss\n",
    "        p = torch.stack([rho, 1 - rho])\n",
    "        q = torch.stack([rho_hat, 1 - rho_hat])\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kl_loss = self.kl_loss(torch.log(q), p)\n",
    "\n",
    "        # Total loss\n",
    "        # print(mse_loss.tolist(), l2_loss.tolist(), kl_loss.tolist())\n",
    "        total_loss = mse_loss + l2_loss + self.sparsity_weight * kl_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9a0857f3-9ac2-4905-88b5-cc961e9772e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ec7ad1c5-58fd-43ce-889e-484c8ae7f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, data_loader, num_epochs, lr=0.01, patience=10):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    autoencoder = autoencoder.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(autoencoder.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # For saving best model\n",
    "    best_train_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_no_improve = 0\n",
    "    epsilon = 1e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_loader, 0):\n",
    "            inputs = batch[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = autoencoder(inputs)\n",
    "            loss = criterion(inputs, decoded)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # total_loss /= len(data_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.6f}')\n",
    "        \n",
    "        # Check if model is better\n",
    "        if best_train_loss - total_loss > epsilon:\n",
    "            best_train_loss = total_loss\n",
    "            best_model = autoencoder.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    autoencoder.load_state_dict(best_model)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "99ad8475-f7b2-4f31-87d2-b1f7f9ca1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssae(ssae, train_loader, num_epochs, lr=0.01, batch_size=256):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    ssae = ssae.to(device)\n",
    "\n",
    "    # Greedy layer-wise training\n",
    "    ## for the first layer\n",
    "    print('Training autoencoder 1')\n",
    "    ae1_best = train_autoencoder(ssae.ae1, train_loader, num_epochs, lr)\n",
    "    ssae.ae1.load_state_dict(ae1_best)\n",
    "\n",
    "    ## prepare data for the next autoencoder\n",
    "    encoded_train_loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.cat([ssae.ae1.encoder(batch[0].to(device)).detach() for batch in train_loader])\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    ## for the second layer\n",
    "    print('Training autoencoder 2')\n",
    "    ae2_best = train_autoencoder(ssae.ae2, encoded_train_loader, num_epochs, lr)\n",
    "    ssae.ae2.load_state_dict(ae2_best)\n",
    "        \n",
    "    # Fine-tuning\n",
    "    print(\"Fine-tuning the entire network\")\n",
    "\n",
    "    # For saving best model\n",
    "    # best_train_loss = float('inf')\n",
    "    # best_model = None\n",
    "    # epochs_no_improve = 0\n",
    "    # epsilon = 1e-2\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ssae.parameters(), lr=lr, momentum=0.95)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ssae.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        outputs_list = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = ssae(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            outputs_list.extend(outputs.argmax(dim=1).tolist())  # Collect max indices of softmax outputs\n",
    "            \n",
    "        ssae.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = ssae(batch_x)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += predicted.eq(batch_y).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Print label distribution in predictions after each epoch\n",
    "        print(f\"Label distribution: {Counter(outputs_list)}\")\n",
    "        print(f'Fine-tuning Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "        # # Check if model is better\n",
    "        # if best_train_loss - total_loss > epsilon:\n",
    "        #     best_train_loss = total_loss\n",
    "        #     best_model = autoencoder.state_dict()\n",
    "        #     epochs_no_improve = 0\n",
    "        # else:\n",
    "        #     epochs_no_improve += 1\n",
    "        \n",
    "        # # Check early stopping condition\n",
    "        # if epochs_no_improve == patience:\n",
    "        #     print(\"Early stopping!\")\n",
    "        #     break\n",
    "\n",
    "    # Load best model\n",
    "    # autoencoder.load_state_dict(best_model)\n",
    "    return ssae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a30c84ab-2449-46e7-8a2d-bb1f1f24185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = df[df.subject == 'sub01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd277917-f762-4495-8237-21c4987286de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83700, 400)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(sub1.window_emg.to_numpy()).reshape(-1, 400).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39b916cd-6405-498f-84af-006f6789060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x(df):\n",
    "    return np.stack(df.window_emg.to_numpy()).reshape(-1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0478274-9b2e-468c-95af-d406d015f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_x(X):\n",
    "    return torch.tensor(X).to(torch.float32).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c176d227-31fe-44be-a83a-6abc896c5d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83700, 400])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorize_x(prepare_x(sub1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fb853f4-8bd0-4271-92cd-983ba53437dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tensorize_x(prepare_x(sub1))\n",
    "y_train = sub1.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c7893fa2-abab-4ed1-868c-99b85ea500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssae = SSAE(400, 100, 50, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b9b4f3f1-1202-4e11-90ff-1a7b92e49b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DataLoader(TensorDataset(X_train.to(torch.float32), torch.FloatTensor(y_train)), batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "41c41691-01c8-4d33-8213-644e7ba3577d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 69.646914\n",
      "Epoch [2/500], Loss: 43.281436\n",
      "Epoch [3/500], Loss: 28.983236\n",
      "Epoch [4/500], Loss: 21.005625\n",
      "Epoch [5/500], Loss: 16.214978\n",
      "Epoch [6/500], Loss: 13.123571\n",
      "Epoch [7/500], Loss: 11.006210\n",
      "Epoch [8/500], Loss: 9.485343\n",
      "Epoch [9/500], Loss: 8.350480\n",
      "Epoch [10/500], Loss: 7.477059\n",
      "Epoch [11/500], Loss: 6.787497\n",
      "Epoch [12/500], Loss: 6.231454\n",
      "Epoch [13/500], Loss: 5.775030\n",
      "Epoch [14/500], Loss: 5.394575\n",
      "Epoch [15/500], Loss: 5.073184\n",
      "Epoch [16/500], Loss: 4.798650\n",
      "Epoch [17/500], Loss: 4.561741\n",
      "Epoch [18/500], Loss: 4.355362\n",
      "Epoch [19/500], Loss: 4.174293\n",
      "Epoch [20/500], Loss: 4.014251\n",
      "Epoch [21/500], Loss: 3.871837\n",
      "Epoch [22/500], Loss: 3.744416\n",
      "Epoch [23/500], Loss: 3.629770\n",
      "Epoch [24/500], Loss: 3.526180\n",
      "Epoch [25/500], Loss: 3.432157\n",
      "Epoch [26/500], Loss: 3.346448\n",
      "Epoch [27/500], Loss: 3.268021\n",
      "Epoch [28/500], Loss: 3.196014\n",
      "Epoch [29/500], Loss: 3.129745\n",
      "Epoch [30/500], Loss: 3.068438\n",
      "Epoch [31/500], Loss: 3.011693\n",
      "Epoch [32/500], Loss: 2.958964\n",
      "Epoch [33/500], Loss: 2.909977\n",
      "Epoch [34/500], Loss: 2.864157\n",
      "Epoch [35/500], Loss: 2.821360\n",
      "Epoch [36/500], Loss: 2.781221\n",
      "Epoch [37/500], Loss: 2.743603\n",
      "Epoch [38/500], Loss: 2.708194\n",
      "Epoch [39/500], Loss: 2.674832\n",
      "Epoch [40/500], Loss: 2.643366\n",
      "Epoch [41/500], Loss: 2.613598\n",
      "Epoch [42/500], Loss: 2.585483\n",
      "Epoch [43/500], Loss: 2.558785\n",
      "Epoch [44/500], Loss: 2.533470\n",
      "Epoch [45/500], Loss: 2.509438\n",
      "Epoch [46/500], Loss: 2.486534\n",
      "Epoch [47/500], Loss: 2.464762\n",
      "Epoch [48/500], Loss: 2.443997\n",
      "Epoch [49/500], Loss: 2.424151\n",
      "Epoch [50/500], Loss: 2.405236\n",
      "Epoch [51/500], Loss: 2.387096\n",
      "Epoch [52/500], Loss: 2.369771\n",
      "Epoch [53/500], Loss: 2.353186\n",
      "Epoch [54/500], Loss: 2.337253\n",
      "Epoch [55/500], Loss: 2.321995\n",
      "Epoch [56/500], Loss: 2.307280\n",
      "Epoch [57/500], Loss: 2.293182\n",
      "Epoch [58/500], Loss: 2.279598\n",
      "Epoch [59/500], Loss: 2.266571\n",
      "Epoch [60/500], Loss: 2.254001\n",
      "Epoch [61/500], Loss: 2.241915\n",
      "Epoch [62/500], Loss: 2.230204\n",
      "Epoch [63/500], Loss: 2.218933\n",
      "Epoch [64/500], Loss: 2.207999\n",
      "Epoch [65/500], Loss: 2.197504\n",
      "Epoch [66/500], Loss: 2.187335\n",
      "Epoch [67/500], Loss: 2.177499\n",
      "Epoch [68/500], Loss: 2.167973\n",
      "Epoch [69/500], Loss: 2.158800\n",
      "Epoch [70/500], Loss: 2.149853\n",
      "Epoch [71/500], Loss: 2.141210\n",
      "Epoch [72/500], Loss: 2.132800\n",
      "Epoch [73/500], Loss: 2.124689\n",
      "Epoch [74/500], Loss: 2.116821\n",
      "Epoch [75/500], Loss: 2.109135\n",
      "Epoch [76/500], Loss: 2.101698\n",
      "Epoch [77/500], Loss: 2.094477\n",
      "Epoch [78/500], Loss: 2.087427\n",
      "Epoch [79/500], Loss: 2.080604\n",
      "Epoch [80/500], Loss: 2.073979\n",
      "Epoch [81/500], Loss: 2.067472\n",
      "Epoch [82/500], Loss: 2.061162\n",
      "Epoch [83/500], Loss: 2.055061\n",
      "Epoch [84/500], Loss: 2.049108\n",
      "Epoch [85/500], Loss: 2.043252\n",
      "Epoch [86/500], Loss: 2.037559\n",
      "Epoch [87/500], Loss: 2.032040\n",
      "Epoch [88/500], Loss: 2.026633\n",
      "Epoch [89/500], Loss: 2.021406\n",
      "Epoch [90/500], Loss: 2.016255\n",
      "Epoch [91/500], Loss: 2.011245\n",
      "Epoch [92/500], Loss: 2.006344\n",
      "Epoch [93/500], Loss: 2.001530\n",
      "Epoch [94/500], Loss: 1.996892\n",
      "Epoch [95/500], Loss: 1.992311\n",
      "Epoch [96/500], Loss: 1.987886\n",
      "Epoch [97/500], Loss: 1.983491\n",
      "Epoch [98/500], Loss: 1.979230\n",
      "Epoch [99/500], Loss: 1.975051\n",
      "Epoch [100/500], Loss: 1.970953\n",
      "Epoch [101/500], Loss: 1.966955\n",
      "Epoch [102/500], Loss: 1.963003\n",
      "Epoch [103/500], Loss: 1.959191\n",
      "Epoch [104/500], Loss: 1.955444\n",
      "Epoch [105/500], Loss: 1.951783\n",
      "Epoch [106/500], Loss: 1.948173\n",
      "Epoch [107/500], Loss: 1.944662\n",
      "Epoch [108/500], Loss: 1.941159\n",
      "Epoch [109/500], Loss: 1.937817\n",
      "Epoch [110/500], Loss: 1.934472\n",
      "Epoch [111/500], Loss: 1.931226\n",
      "Epoch [112/500], Loss: 1.927995\n",
      "Epoch [113/500], Loss: 1.924853\n",
      "Epoch [114/500], Loss: 1.921807\n",
      "Epoch [115/500], Loss: 1.918761\n",
      "Epoch [116/500], Loss: 1.915831\n",
      "Epoch [117/500], Loss: 1.912893\n",
      "Epoch [118/500], Loss: 1.910054\n",
      "Epoch [119/500], Loss: 1.907284\n",
      "Epoch [120/500], Loss: 1.904536\n",
      "Epoch [121/500], Loss: 1.901780\n",
      "Epoch [122/500], Loss: 1.899123\n",
      "Epoch [123/500], Loss: 1.896484\n",
      "Epoch [124/500], Loss: 1.893912\n",
      "Epoch [125/500], Loss: 1.891386\n",
      "Epoch [126/500], Loss: 1.888924\n",
      "Epoch [127/500], Loss: 1.886471\n",
      "Epoch [128/500], Loss: 1.884105\n",
      "Epoch [129/500], Loss: 1.881745\n",
      "Epoch [130/500], Loss: 1.879370\n",
      "Epoch [131/500], Loss: 1.877087\n",
      "Epoch [132/500], Loss: 1.874858\n",
      "Epoch [133/500], Loss: 1.872630\n",
      "Epoch [134/500], Loss: 1.870553\n",
      "Epoch [135/500], Loss: 1.868330\n",
      "Epoch [136/500], Loss: 1.866208\n",
      "Epoch [137/500], Loss: 1.864105\n",
      "Epoch [138/500], Loss: 1.862077\n",
      "Epoch [139/500], Loss: 1.860071\n",
      "Epoch [140/500], Loss: 1.858089\n",
      "Epoch [141/500], Loss: 1.856093\n",
      "Epoch [142/500], Loss: 1.854222\n",
      "Epoch [143/500], Loss: 1.852321\n",
      "Epoch [144/500], Loss: 1.850444\n",
      "Epoch [145/500], Loss: 1.848607\n",
      "Epoch [146/500], Loss: 1.846773\n",
      "Epoch [147/500], Loss: 1.845016\n",
      "Epoch [148/500], Loss: 1.843232\n",
      "Epoch [149/500], Loss: 1.841473\n",
      "Epoch [150/500], Loss: 1.839787\n",
      "Epoch [151/500], Loss: 1.838080\n",
      "Epoch [152/500], Loss: 1.836391\n",
      "Epoch [153/500], Loss: 1.834810\n",
      "Epoch [154/500], Loss: 1.833149\n",
      "Epoch [155/500], Loss: 1.831576\n",
      "Epoch [156/500], Loss: 1.829992\n",
      "Epoch [157/500], Loss: 1.828439\n",
      "Epoch [158/500], Loss: 1.826883\n",
      "Epoch [159/500], Loss: 1.825389\n",
      "Epoch [160/500], Loss: 1.823888\n",
      "Epoch [161/500], Loss: 1.822401\n",
      "Epoch [162/500], Loss: 1.820965\n",
      "Epoch [163/500], Loss: 1.819515\n",
      "Epoch [164/500], Loss: 1.818124\n",
      "Epoch [165/500], Loss: 1.816688\n",
      "Epoch [166/500], Loss: 1.815311\n",
      "Epoch [167/500], Loss: 1.813953\n",
      "Epoch [168/500], Loss: 1.812586\n",
      "Epoch [169/500], Loss: 1.811285\n",
      "Epoch [170/500], Loss: 1.809943\n",
      "Epoch [171/500], Loss: 1.808662\n",
      "Epoch [172/500], Loss: 1.807372\n",
      "Epoch [173/500], Loss: 1.806102\n",
      "Epoch [174/500], Loss: 1.804875\n",
      "Epoch [175/500], Loss: 1.803616\n",
      "Epoch [176/500], Loss: 1.802426\n",
      "Epoch [177/500], Loss: 1.801192\n",
      "Epoch [178/500], Loss: 1.800002\n",
      "Epoch [179/500], Loss: 1.798833\n",
      "Epoch [180/500], Loss: 1.797665\n",
      "Epoch [181/500], Loss: 1.796491\n",
      "Epoch [182/500], Loss: 1.795350\n",
      "Epoch [183/500], Loss: 1.794257\n",
      "Epoch [184/500], Loss: 1.793097\n",
      "Epoch [185/500], Loss: 1.792048\n",
      "Epoch [186/500], Loss: 1.790903\n",
      "Epoch [187/500], Loss: 1.789844\n",
      "Epoch [188/500], Loss: 1.788806\n",
      "Epoch [189/500], Loss: 1.787739\n",
      "Epoch [190/500], Loss: 1.786702\n",
      "Epoch [191/500], Loss: 1.785710\n",
      "Epoch [192/500], Loss: 1.784635\n",
      "Epoch [193/500], Loss: 1.783631\n",
      "Epoch [194/500], Loss: 1.782695\n",
      "Epoch [195/500], Loss: 1.781647\n",
      "Epoch [196/500], Loss: 1.780666\n",
      "Epoch [197/500], Loss: 1.779745\n",
      "Epoch [198/500], Loss: 1.778756\n",
      "Epoch [199/500], Loss: 1.777859\n",
      "Epoch [200/500], Loss: 1.776896\n",
      "Epoch [201/500], Loss: 1.775953\n",
      "Epoch [202/500], Loss: 1.775034\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(ssae.ae1, a, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "17967200-60b1-44f9-8d16-20a011b7e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.cat([ssae.ae1.encoder(batch[0].to('mps')).detach() for batch in a])\n",
    "    ),\n",
    "    batch_size=256,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fdf7c84e-392c-499b-9c33-21ab5a71dd84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.240194\n",
      "Epoch [2/50], Loss: 1.041446\n",
      "Epoch [3/50], Loss: 0.337914\n",
      "Epoch [4/50], Loss: 0.121366\n",
      "Epoch [5/50], Loss: 0.056959\n",
      "Epoch [6/50], Loss: 0.038076\n",
      "Epoch [7/50], Loss: 0.032545\n",
      "Epoch [8/50], Loss: 0.030916\n",
      "Epoch [9/50], Loss: 0.030432\n",
      "Epoch [10/50], Loss: 0.030286\n",
      "Epoch [11/50], Loss: 0.030242\n",
      "Epoch [12/50], Loss: 0.030229\n",
      "Epoch [13/50], Loss: 0.030224\n",
      "Epoch [14/50], Loss: 0.030223\n",
      "Epoch [15/50], Loss: 0.030223\n",
      "Epoch [16/50], Loss: 0.030222\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train_autoencoder(ssae.ae2, b, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a089133f-fd87-4774-95c5-3f0daefab3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 400])\n",
      "torch.Size([100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([50, 100])\n",
      "torch.Size([50])\n",
      "torch.Size([100, 50])\n",
      "torch.Size([100])\n",
      "torch.Size([9, 50])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "for i in ssae.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cf5cc4d3-45ad-45a7-b0ab-af2465fc8b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CH', 'EX', 'FL', 'GR', 'IN', 'OH', 'PR', 'RT', 'SU'], dtype=object)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9bce51a9-cb5c-49d9-8dca-4ede11f9d033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0156,  0.0391, -0.0234,  ..., -0.0391, -0.0312, -0.0469],\n",
       "         [-0.0938, -0.0312, -0.0156,  ...,  0.0000,  0.0781,  0.1094],\n",
       "         [ 0.0078,  0.0078, -0.0156,  ..., -0.2656, -0.0312, -0.0156],\n",
       "         ...,\n",
       "         [-0.0078, -0.0078,  0.0000,  ..., -0.0078, -0.0078,  0.0000],\n",
       "         [-0.0547, -0.0078, -0.0078,  ...,  0.0312,  0.0000, -0.0234],\n",
       "         [-0.0078, -0.0156,  0.0000,  ..., -0.0156, -0.0234, -0.0078]],\n",
       "        device='mps:0'),\n",
       " tensor([7., 3., 2., 8., 6., 1., 5., 4., 1., 5., 4., 8., 7., 0., 3., 0., 0., 7.,\n",
       "         2., 7., 8., 7., 0., 3., 4., 0., 4., 8., 3., 7., 8., 3., 2., 7., 1., 5.,\n",
       "         7., 5., 5., 1., 5., 2., 4., 8., 3., 3., 2., 3., 3., 3., 1., 1., 8., 3.,\n",
       "         1., 0., 2., 6., 7., 5., 8., 7., 5., 2., 7., 3., 6., 6., 2., 0., 1., 2.,\n",
       "         1., 7., 5., 3., 1., 2., 2., 5., 0., 0., 8., 2., 7., 5., 5., 2., 5., 5.,\n",
       "         5., 8., 8., 8., 5., 8., 7., 8., 8., 3., 7., 4., 0., 7., 0., 8., 1., 7.,\n",
       "         3., 5., 1., 2., 7., 0., 0., 6., 4., 6., 2., 3., 0., 2., 1., 6., 6., 3.,\n",
       "         8., 6., 5., 2., 5., 0., 6., 4., 8., 2., 3., 7., 1., 2., 8., 4., 0., 3.,\n",
       "         6., 4., 2., 7., 1., 5., 7., 4., 3., 7., 2., 7., 6., 5., 1., 2., 8., 8.,\n",
       "         8., 1., 4., 0., 5., 5., 8., 2., 0., 6., 2., 2., 5., 5., 3., 1., 4., 1.,\n",
       "         5., 2., 5., 8., 3., 0., 8., 1., 6., 8., 1., 8., 7., 8., 1., 5., 6., 1.,\n",
       "         7., 2., 3., 0., 6., 1., 0., 4., 8., 3., 0., 8., 0., 8., 5., 2., 4., 4.,\n",
       "         6., 2., 2., 1., 0., 3., 1., 4., 3., 3., 1., 4., 0., 4., 4., 8., 7., 0.,\n",
       "         2., 4., 6., 8., 4., 0., 3., 1., 1., 0., 4., 5., 7., 1., 2., 3., 3., 5.,\n",
       "         4., 8., 1., 8.])]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "76e3b6c3-249d-4659-b2d5-d9cc822b34b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: Counter({4: 59562, 3: 24138})\n",
      "Fine-tuning Epoch [1/50], Loss: 2.1975\n",
      "Label distribution: Counter({3: 43843, 4: 27458, 5: 12399})\n",
      "Fine-tuning Epoch [2/50], Loss: 2.1973\n",
      "Label distribution: Counter({4: 49685, 3: 20889, 5: 13126})\n",
      "Fine-tuning Epoch [3/50], Loss: 2.1973\n",
      "Label distribution: Counter({3: 45072, 4: 30492, 5: 5564, 0: 2572})\n",
      "Fine-tuning Epoch [4/50], Loss: 2.1973\n",
      "Label distribution: Counter({4: 79634, 1: 1901, 0: 1185, 3: 752, 7: 228})\n",
      "Fine-tuning Epoch [5/50], Loss: 2.1972\n",
      "Label distribution: Counter({3: 32313, 7: 20463, 1: 18843, 0: 8774, 4: 3306, 5: 1})\n",
      "Fine-tuning Epoch [6/50], Loss: 2.1972\n",
      "Label distribution: Counter({3: 28872, 0: 17520, 6: 12608, 7: 9990, 5: 8995, 2: 5121, 4: 594})\n",
      "Fine-tuning Epoch [7/50], Loss: 2.1973\n",
      "Label distribution: Counter({5: 35388, 7: 21976, 6: 13649, 0: 12645, 4: 26, 8: 10, 2: 6})\n",
      "Fine-tuning Epoch [8/50], Loss: 2.1973\n",
      "Label distribution: Counter({7: 35627, 5: 26898, 4: 6170, 2: 5674, 6: 3761, 0: 2902, 3: 1927, 8: 740, 1: 1})\n",
      "Fine-tuning Epoch [9/50], Loss: 2.1973\n",
      "Label distribution: Counter({2: 24791, 4: 22928, 7: 14328, 6: 11145, 0: 7396, 1: 3103, 5: 8, 8: 1})\n",
      "Fine-tuning Epoch [10/50], Loss: 2.1973\n",
      "Label distribution: Counter({0: 32514, 4: 29930, 2: 15284, 1: 3037, 5: 2414, 7: 394, 3: 127})\n",
      "Fine-tuning Epoch [11/50], Loss: 2.1973\n",
      "Label distribution: Counter({1: 28338, 4: 16362, 3: 13334, 7: 12693, 6: 12683, 2: 290})\n",
      "Fine-tuning Epoch [12/50], Loss: 2.1972\n",
      "Label distribution: Counter({0: 36817, 5: 17786, 4: 10536, 2: 9425, 3: 5323, 8: 2063, 7: 1750})\n",
      "Fine-tuning Epoch [13/50], Loss: 2.1972\n",
      "Label distribution: Counter({7: 28908, 4: 28674, 2: 15758, 8: 5617, 1: 3421, 5: 919, 6: 401, 0: 2})\n",
      "Fine-tuning Epoch [14/50], Loss: 2.1973\n",
      "Label distribution: Counter({8: 70684, 6: 9669, 4: 2204, 3: 1129, 5: 14})\n",
      "Fine-tuning Epoch [15/50], Loss: 2.1972\n",
      "Label distribution: Counter({6: 51439, 1: 8837, 3: 6200, 8: 6070, 4: 5850, 2: 5304})\n",
      "Fine-tuning Epoch [16/50], Loss: 2.1973\n",
      "Label distribution: Counter({7: 22250, 1: 20450, 6: 20246, 2: 10955, 8: 7455, 4: 1663, 0: 602, 5: 79})\n",
      "Fine-tuning Epoch [17/50], Loss: 2.1973\n",
      "Label distribution: Counter({6: 33894, 4: 24929, 0: 13915, 3: 10062, 2: 848, 7: 52})\n",
      "Fine-tuning Epoch [18/50], Loss: 2.1972\n",
      "Label distribution: Counter({3: 29196, 1: 19121, 6: 17343, 4: 15962, 0: 1448, 5: 619, 8: 8, 2: 3})\n",
      "Fine-tuning Epoch [19/50], Loss: 2.1973\n",
      "Label distribution: Counter({5: 27448, 8: 17503, 1: 15233, 6: 14956, 0: 6724, 2: 1836})\n",
      "Fine-tuning Epoch [20/50], Loss: 2.1973\n",
      "Label distribution: Counter({8: 27042, 5: 14583, 6: 14288, 4: 12274, 2: 9655, 3: 4568, 1: 1290})\n",
      "Fine-tuning Epoch [21/50], Loss: 2.1972\n",
      "Label distribution: Counter({1: 22855, 3: 16440, 2: 15088, 7: 14785, 0: 14521, 6: 11})\n",
      "Fine-tuning Epoch [22/50], Loss: 2.1973\n",
      "Label distribution: Counter({3: 38266, 6: 13441, 8: 12997, 4: 11320, 2: 3197, 7: 2355, 5: 1194, 0: 918, 1: 12})\n",
      "Fine-tuning Epoch [23/50], Loss: 2.1973\n",
      "Label distribution: Counter({6: 27337, 3: 18843, 7: 17692, 0: 17250, 4: 2462, 2: 71, 1: 45})\n",
      "Fine-tuning Epoch [24/50], Loss: 2.1973\n",
      "Label distribution: Counter({5: 33266, 8: 23196, 4: 15258, 7: 10868, 6: 895, 2: 216, 3: 1})\n",
      "Fine-tuning Epoch [25/50], Loss: 2.1973\n",
      "Label distribution: Counter({0: 31155, 8: 15497, 1: 12985, 3: 9778, 6: 9043, 4: 2772, 7: 1673, 2: 797})\n",
      "Fine-tuning Epoch [26/50], Loss: 2.1972\n",
      "Label distribution: Counter({3: 31679, 7: 15587, 4: 13573, 2: 11423, 5: 11015, 1: 390, 8: 31, 6: 2})\n",
      "Fine-tuning Epoch [27/50], Loss: 2.1973\n",
      "Label distribution: Counter({1: 55864, 6: 14681, 7: 8839, 8: 1906, 5: 1520, 4: 877, 2: 7, 3: 5, 0: 1})\n",
      "Fine-tuning Epoch [28/50], Loss: 2.1972\n",
      "Label distribution: Counter({2: 50565, 7: 9076, 0: 7677, 3: 6929, 4: 3181, 1: 3133, 6: 2209, 5: 757, 8: 173})\n",
      "Fine-tuning Epoch [29/50], Loss: 2.1973\n",
      "Label distribution: Counter({4: 34542, 5: 16005, 6: 13375, 8: 8588, 3: 6795, 7: 4387, 2: 6, 0: 2})\n",
      "Fine-tuning Epoch [30/50], Loss: 2.1972\n",
      "Label distribution: Counter({6: 37029, 0: 22331, 1: 16868, 8: 6253, 7: 1217, 3: 2})\n",
      "Fine-tuning Epoch [31/50], Loss: 2.1973\n",
      "Label distribution: Counter({7: 31020, 4: 20881, 0: 17399, 3: 6105, 8: 3537, 1: 2996, 5: 1751, 2: 11})\n",
      "Fine-tuning Epoch [32/50], Loss: 2.1972\n",
      "Label distribution: Counter({1: 32100, 3: 17855, 6: 13054, 5: 11157, 0: 5558, 2: 3669, 4: 306, 7: 1})\n",
      "Fine-tuning Epoch [33/50], Loss: 2.1972\n",
      "Label distribution: Counter({1: 35158, 3: 34144, 4: 8834, 5: 2416, 8: 1334, 6: 1267, 7: 535, 0: 12})\n",
      "Fine-tuning Epoch [34/50], Loss: 2.1973\n",
      "Label distribution: Counter({5: 34465, 2: 27904, 8: 8891, 6: 6228, 0: 4269, 7: 1941, 1: 2})\n",
      "Fine-tuning Epoch [35/50], Loss: 2.1972\n",
      "Label distribution: Counter({3: 27265, 4: 18565, 5: 16188, 7: 10113, 6: 9692, 2: 1560, 1: 316, 0: 1})\n",
      "Fine-tuning Epoch [36/50], Loss: 2.1972\n",
      "Label distribution: Counter({1: 48982, 3: 10096, 5: 9871, 7: 8308, 4: 2880, 8: 1916, 0: 1619, 2: 24, 6: 4})\n",
      "Fine-tuning Epoch [37/50], Loss: 2.1973\n",
      "Label distribution: Counter({8: 35326, 7: 23799, 6: 21778, 2: 2731, 3: 66})\n",
      "Fine-tuning Epoch [38/50], Loss: 2.1973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[300], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m a:\n\u001b[0;32m---> 18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ssae(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssae = SSAE(400, 100, 50, 9)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ssae.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters_to_optimize = list(ssae.ae1.encoder.parameters()) + \\\n",
    "                         list(ssae.ae2.encoder.parameters()) + \\\n",
    "                         list(ssae.classifier.parameters())\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(50):\n",
    "    ssae.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    outputs_list = []\n",
    "    \n",
    "    for inputs, labels in a:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssae(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        outputs_list.extend(outputs.argmax(dim=1).tolist())\n",
    "\n",
    "    print(f\"Label distribution: {Counter(outputs_list)}\")\n",
    "    print(f'Fine-tuning Epoch [{epoch+1}/{50}], Loss: {total_loss/len(a):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "990a5860-f8f8-4cd8-89fb-c9f481ecc534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training autoencoder 1\n",
      "Epoch [1/25], Loss: 68.661787\n",
      "Epoch [2/25], Loss: 42.810947\n",
      "Epoch [3/25], Loss: 28.779858\n",
      "Epoch [4/25], Loss: 20.918176\n",
      "Epoch [5/25], Loss: 16.177715\n",
      "Epoch [6/25], Loss: 13.109153\n",
      "Epoch [7/25], Loss: 11.002871\n",
      "Epoch [8/25], Loss: 9.487515\n",
      "Epoch [9/25], Loss: 8.355364\n",
      "Epoch [10/25], Loss: 7.483260\n",
      "Epoch [11/25], Loss: 6.794303\n",
      "Epoch [12/25], Loss: 6.238494\n",
      "Epoch [13/25], Loss: 5.781980\n",
      "Epoch [14/25], Loss: 5.401326\n",
      "Epoch [15/25], Loss: 5.079731\n",
      "Epoch [16/25], Loss: 4.804880\n",
      "Epoch [17/25], Loss: 4.567663\n",
      "Epoch [18/25], Loss: 4.361047\n",
      "Epoch [19/25], Loss: 4.179673\n",
      "Epoch [20/25], Loss: 4.019310\n",
      "Epoch [21/25], Loss: 3.876697\n",
      "Epoch [22/25], Loss: 3.749069\n",
      "Epoch [23/25], Loss: 3.634180\n",
      "Epoch [24/25], Loss: 3.530437\n",
      "Epoch [25/25], Loss: 3.436159\n",
      "Training autoencoder 2\n",
      "Epoch [1/25], Loss: 2.145752\n",
      "Epoch [2/25], Loss: 0.694320\n",
      "Epoch [3/25], Loss: 0.241208\n",
      "Epoch [4/25], Loss: 0.098201\n",
      "Epoch [5/25], Loss: 0.053025\n",
      "Epoch [6/25], Loss: 0.038756\n",
      "Epoch [7/25], Loss: 0.034245\n",
      "Epoch [8/25], Loss: 0.032814\n",
      "Epoch [9/25], Loss: 0.032361\n",
      "Epoch [10/25], Loss: 0.032216\n",
      "Epoch [11/25], Loss: 0.032170\n",
      "Epoch [12/25], Loss: 0.032155\n",
      "Epoch [13/25], Loss: 0.032150\n",
      "Epoch [14/25], Loss: 0.032149\n",
      "Epoch [15/25], Loss: 0.032148\n",
      "Epoch [16/25], Loss: 0.032148\n",
      "Early stopping!\n",
      "Fine-tuning the entire network\n",
      "Label distribution: Counter({0: 51734, 3: 29388, 5: 2578})\n",
      "Fine-tuning Epoch [1/25], Loss: 2.1975, Accuracy: 0.1111\n",
      "Label distribution: Counter({5: 35113, 3: 21763, 0: 17560, 8: 6285, 6: 2979})\n",
      "Fine-tuning Epoch [2/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({0: 34148, 5: 15429, 3: 15090, 8: 13476, 1: 5557})\n",
      "Fine-tuning Epoch [3/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({6: 30304, 5: 30018, 2: 12229, 4: 7068, 1: 3728, 8: 352, 7: 1})\n",
      "Fine-tuning Epoch [4/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({3: 31282, 7: 23639, 5: 18009, 6: 7260, 1: 3408, 4: 102})\n",
      "Fine-tuning Epoch [5/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({8: 36122, 7: 19402, 1: 14785, 4: 10640, 0: 2482, 3: 263, 2: 5, 5: 1})\n",
      "Fine-tuning Epoch [6/25], Loss: 2.1973, Accuracy: 0.1317\n",
      "Label distribution: Counter({2: 33635, 4: 14737, 3: 12019, 5: 11258, 8: 8720, 1: 2736, 0: 595})\n",
      "Fine-tuning Epoch [7/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({7: 33925, 5: 19498, 0: 16193, 6: 13281, 3: 803})\n",
      "Fine-tuning Epoch [8/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({8: 45302, 2: 27351, 3: 7712, 7: 3335})\n",
      "Fine-tuning Epoch [9/25], Loss: 2.1973, Accuracy: 0.1194\n",
      "Label distribution: Counter({3: 46215, 7: 20755, 1: 15749, 8: 981})\n",
      "Fine-tuning Epoch [10/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({7: 27443, 5: 23299, 8: 15029, 2: 14046, 4: 2720, 6: 1163})\n",
      "Fine-tuning Epoch [11/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({6: 37013, 3: 26551, 0: 10050, 1: 8708, 4: 864, 8: 503, 5: 11})\n",
      "Fine-tuning Epoch [12/25], Loss: 2.1973, Accuracy: 0.1248\n",
      "Label distribution: Counter({8: 28326, 2: 19530, 4: 18238, 0: 16015, 6: 1303, 5: 288})\n",
      "Fine-tuning Epoch [13/25], Loss: 2.1973, Accuracy: 0.1101\n",
      "Label distribution: Counter({5: 49685, 7: 17351, 1: 7553, 3: 6031, 4: 2941, 0: 135, 6: 4})\n",
      "Fine-tuning Epoch [14/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({7: 35358, 2: 19967, 6: 14924, 4: 13451})\n",
      "Fine-tuning Epoch [15/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({8: 35698, 3: 15993, 5: 15143, 6: 6531, 0: 6316, 4: 3352, 1: 667})\n",
      "Fine-tuning Epoch [16/25], Loss: 2.1973, Accuracy: 0.1180\n",
      "Label distribution: Counter({2: 29991, 1: 28226, 7: 8837, 6: 8339, 8: 7873, 0: 333, 4: 101})\n",
      "Fine-tuning Epoch [17/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({5: 53490, 4: 16204, 0: 7943, 1: 3772, 8: 2286, 3: 5})\n",
      "Fine-tuning Epoch [18/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({0: 25270, 4: 20737, 7: 18942, 8: 11469, 5: 5711, 6: 1571})\n",
      "Fine-tuning Epoch [19/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({6: 36255, 3: 27386, 4: 9387, 1: 8472, 2: 2029, 8: 98, 5: 73})\n",
      "Fine-tuning Epoch [20/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({2: 59427, 6: 13080, 1: 6546, 4: 2669, 5: 1977, 7: 1})\n",
      "Fine-tuning Epoch [21/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({5: 27592, 2: 11938, 0: 11715, 8: 11567, 3: 11478, 1: 4451, 7: 3426, 4: 1533})\n",
      "Fine-tuning Epoch [22/25], Loss: 2.1973, Accuracy: 0.1111\n",
      "Label distribution: Counter({4: 39078, 0: 19442, 2: 14702, 8: 9423, 3: 980, 6: 60, 7: 15})\n",
      "Fine-tuning Epoch [23/25], Loss: 2.1973, Accuracy: 0.1105\n",
      "Label distribution: Counter({3: 50060, 7: 23260, 1: 5635, 8: 3640, 6: 699, 0: 406})\n",
      "Fine-tuning Epoch [24/25], Loss: 2.1973, Accuracy: 0.1156\n",
      "Label distribution: Counter({2: 17956, 7: 17168, 0: 17103, 8: 11395, 5: 8501, 4: 6998, 3: 4579})\n",
      "Fine-tuning Epoch [25/25], Loss: 2.1973, Accuracy: 0.1111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssae = SSAE(400, 100, 50, 9)\n",
    "ssae = train_ssae(ssae, a, num_epochs=25)\n",
    "ssae.load_state_dict(best_ssae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1a1ec671-bf2f-451d-b04e-d6d40601aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ssae(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5ff6af91-df05-4ce2-859b-5099ce638c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = y_pred.max(1)[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "26611e27-6697-47c6-a4b9-150db9933045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecac20-9a39-4ab1-bbb7-dfe18eb45eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
